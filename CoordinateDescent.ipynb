{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoordinateDescent_V3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# $\\color{red} {\\text{Coordinate Descent}}$\n",
        "\n",
        "> ## $\\color{cyan}{\\text{What is Coordinate Descent?}}$\n",
        "\n",
        ">> **Coordinate descent** is an optimization algorithm that successively minimizes along coordinate directions to find the minimum of a function. At each iteration, the algorithm determines a coordinate or coordinate block via a coordinate selection rule, then exactly or inexactly minimizes over the corresponding coordinate hyperplane while fixing all other coordinates or coordinate blocks. A line search along the coordinate direction can be performed at the current iterate to determine the appropriate step size. Coordinate descent is applicable in both differentiable and derivative-free contexts.\n",
        "\n",
        "> ## $\\color{cyan}{\\text{What types are done here?}}$\n",
        "\n",
        ">> * random coordinate descent\n",
        ">> * cyclic coordinate descent\n",
        ">> * adaptive coordinate descent"
      ],
      "metadata": {
        "id": "UlA6DyiR6DgF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVZyrkZC06hi"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn import preprocessing\n",
        "from scipy.optimize import fmin\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pp\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lf(wi, x, y, w,idx = 'N'):\n",
        "  loss = 0\n",
        "  if idx != 'N':\n",
        "    w[idx] = wi\n",
        "  for i in range(len(y)):\n",
        "    if y[i] == 1:\n",
        "      loss += np.log(1.0 + np.exp(-np.dot(w,x[i,:])))\n",
        "    else:\n",
        "      loss += np.log(1.0 + np.exp(np.dot(w,x[i,:])))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "U7ykQCxXu3Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# successfully replicated score function now with w as parameter\n",
        "def test_score(x, y, w):\n",
        "  pred_y = np.zeros(len(y))\n",
        "  ll = 0\n",
        "  err = 0\n",
        "  for i in range(len(y)):\n",
        "    one = 1.0/(1.0 + np.exp(-np.dot(w,x[i,:])))\n",
        "    zero = 1.0 - one\n",
        "    pred_y[i] = 1 if one > zero else 0\n",
        "    if pred_y[i] != y[i]:\n",
        "      err = err + 1\n",
        "  return (1 - err/len(y))"
      ],
      "metadata": {
        "id": "Iq8mA4PfBAhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient function - V2\n",
        "# will calculate gradient for all values at once\n",
        "def gradient(x,y,w):\n",
        "  # returns a (x.length,1) array of the P(y=1) for each x \n",
        "  pred = 1.0/(1.0 + np.exp(-np.dot(x,w)))\n",
        "  pred = np.reshape(pred,(104,1))\n",
        "  grad = np.dot(x.T,np.subtract(pred, y))\n",
        "  return grad"
      ],
      "metadata": {
        "id": "OWrSPrUtNgQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how to pick idx to minimize\n",
        "def adaptive_idx(x,y,w):\n",
        "  grad = gradient(x,y,w)\n",
        "  max_i = np.argmax(abs(grad))\n",
        "  return max_i"
      ],
      "metadata": {
        "id": "U6aHDHcDSCKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minimize the loss function\n",
        "# might redo so i have the learning rate\n",
        "def update_wi(x, y, w, wi, index):\n",
        "  updated_wi = fmin(lf,args=(x,y,w,index),x0 = wi, disp = False)\n",
        "  return updated_wi"
      ],
      "metadata": {
        "id": "g-nPktyMT0_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "df = load_wine(as_frame = True)\n",
        "# concatenating horizontally\n",
        "data = pd.concat([df.data, df.target], axis = 1)\n",
        "# slicing the data so only those of class 0 and 1 are included (shuffling included)\n",
        "data = data.loc[data['target'] != 2].sample(frac = 1)\n",
        "# get training and testing datasets\n",
        "train, test = train_test_split(data,test_size = 0.2)\n",
        "train_x = np.split(train,[13,14],axis = 1)[0].to_numpy()\n",
        "train_y = np.split(train,[13,14],axis = 1)[1].to_numpy()\n",
        "test_x = np.split(test,[13,14],axis = 1)[0].to_numpy()\n",
        "test_y = np.split(test,[13,14],axis = 1)[1].to_numpy()"
      ],
      "metadata": {
        "id": "nRjSBsIG4hRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline logistic regression - test\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#need to have penalty or else will overfit\n",
        "lr = LogisticRegression(solver='lbfgs',max_iter=1000000,C=100000)\n",
        "lr.fit(train_x, np.ravel(train_y))\n",
        "score_star = lr.score(test_x,np.ravel(test_y))\n",
        "print(score_star)\n",
        "lr_pred_prob_test = lr.predict_proba(test_x)\n",
        "lr_pred_prob_train = lr.predict_proba(train_x)\n",
        "w_star = lr.coef_\n",
        "b_star = lr.intercept_\n",
        "# combine the w* and b* to get w'*\n",
        "w_p_star = np.append(b_star, w_star)\n",
        "print(w_p_star)\n",
        "# calculate the final loss (log_loss function + maybe self-implemented function)\n",
        "lr_loss_test = log_loss(np.ravel(test_y), lr_pred_prob_test, normalize=False)\n",
        "lr_loss_train = log_loss(np.ravel(train_y), lr_pred_prob_train, normalize=False)\n",
        "print(lr.n_iter_)\n",
        "print(lr_loss_test)\n",
        "print(lr_loss_train)"
      ],
      "metadata": {
        "id": "HJkRqhKW7uiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add intercept column\n",
        "train_int = np.ones((len(train_x),1))\n",
        "test_int = np.ones((len(test_x),1))\n",
        "train_x_b = np.hstack((train_int,train_x))\n",
        "test_x_b = np.hstack((test_int,test_x))"
      ],
      "metadata": {
        "id": "HkRRAaGyaudc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the data\n",
        "train_x_norm = preprocessing.normalize(train_x_b, norm = 'l2')\n",
        "test_x_norm = preprocessing.normalize(test_x_b, norm = 'l2')"
      ],
      "metadata": {
        "id": "v4Jqqpy0cYd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random coordinate descent\n",
        "rcd_loss = []\n",
        "w_rcd = np.random.rand(14)\n",
        "rcd_ct = 0\n",
        "cur_rcd_loss = 10000\n",
        "max_iter = 10000\n",
        "while cur_rcd_loss > 1 and rcd_ct < max_iter:\n",
        "   idx = randint(0,13)\n",
        "   updated_wi = update_wi(train_x_norm,train_y,w_rcd,w_rcd[idx],idx)[0]\n",
        "   w_rcd[idx] = updated_wi\n",
        "   cur_rcd_loss = lf(1,train_x_norm,train_y,w_rcd)\n",
        "   rcd_loss.append(cur_rcd_loss)\n",
        "   print(cur_rcd_loss)\n",
        "   rcd_ct += 1"
      ],
      "metadata": {
        "id": "6FmDqtMx7d4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_score(test_x_b,test_y,w_rcd))\n",
        "print(lf(1,test_x_norm,test_y,w_rcd))\n",
        "print(w_rcd)\n",
        "print(rcd_ct)"
      ],
      "metadata": {
        "id": "x0X9yZGXDGNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cyclic coordinate descent\n",
        "ccd_loss = []\n",
        "w_ccd = np.random.rand(14)\n",
        "ccd_ct = 0\n",
        "cur_ccd_loss = 10000\n",
        "idx = 0\n",
        "max_iter = 10000\n",
        "cur_iter = 0\n",
        "while cur_ccd_loss > 1 and ccd_ct < max_iter:\n",
        "   idx = idx + 1 if idx + 1 < 14 else 0\n",
        "   updated_wi = update_wi(train_x_norm,train_y,w_ccd,w_ccd[idx],idx)[0]\n",
        "   w_ccd[idx] = updated_wi\n",
        "   cur_ccd_loss = lf(1,train_x_norm,train_y,w_ccd)\n",
        "   ccd_loss.append(cur_ccd_loss)\n",
        "   print(cur_ccd_loss)\n",
        "   ccd_ct += 1"
      ],
      "metadata": {
        "id": "anFIewTL7gp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_score(test_x_b,test_y,w_ccd))\n",
        "print(lf(1,test_x_norm,test_y,w_ccd))\n",
        "print(w_ccd)\n",
        "print(ccd_ct)"
      ],
      "metadata": {
        "id": "euVXwMLuCd5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adaptive coordinate descent\n",
        "acd_loss = []\n",
        "acd_dict = {}\n",
        "w_acd = np.random.rand(14)\n",
        "cur_acd_loss = 10000\n",
        "acd_ct = 0\n",
        "max_iter = 10000\n",
        "while cur_acd_loss > 1 and acd_ct < max_iter:\n",
        "  idx = adaptive_idx(train_x_norm,train_y,w_acd)\n",
        "  if (idx in acd_dict):\n",
        "    acd_dict[idx] += 1\n",
        "  else:\n",
        "    acd_dict[idx] = 1\n",
        "  updated_wi = update_wi(train_x_norm, train_y,w_acd,w_acd[idx],idx)[0]\n",
        "  w_acd[idx] = updated_wi\n",
        "  cur_acd_loss = lf(1,train_x_norm,train_y,w_acd)\n",
        "  acd_loss.append(cur_acd_loss)\n",
        "  print(cur_acd_loss)\n",
        "  acd_ct += 1"
      ],
      "metadata": {
        "id": "Nx5ZL02G7ifu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_score(test_x_b,test_y,w_acd))\n",
        "print(lf(1,test_x_norm,test_y,w_acd))\n",
        "print(w_acd)\n",
        "print(acd_ct)\n",
        "print(acd_dict)"
      ],
      "metadata": {
        "id": "VrCG7y6WUlV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create graph of each side by side\n",
        "pp.xlabel('Number of iterations')\n",
        "pp.ylabel('Training loss')\n",
        "pp.xlim([0, 4000])\n",
        "pp.plot(rcd_loss)\n",
        "pp.plot(ccd_loss)\n",
        "pp.plot(acd_loss)\n",
        "pp.legend(['RCD','CCD','ACD'])\n",
        "pp.show()"
      ],
      "metadata": {
        "id": "XdLVQLtcHR0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}